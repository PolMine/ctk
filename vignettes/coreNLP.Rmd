---
title: "Corpus Preparation using CoreNLP"
author: "Andreas BlÃ¤tte (andreas.blaette@uni-due.de)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Corpus Preparation using CoreNLP}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Setting parameters and global variables

The java.parameters need to be set before loading the packages: A JVM is initialized when ctk imports from then openNLP package, and if the JVM is not provided with sufficient memory on this occasion, the later initialization of corenlp will hit the memory limit.
  
```{r}
library(ctk)
options(java.parameters = "-Xmx4g")  
options(ctk.stanfordDir = "/opt/stanford-corenlp/stanford-corenlp-full-2017-06-09")
options(ctk.propertiesFile = "/opt/stanford-corenlp/StanfordCoreNLP-german.properties")

noCores <- parallel::detectCores() - 1
```


# Initialize Pipe

```{r}
P <- Pipe$new(dir = "~/Lab/tmp/testpipe/")
```

# Generate Basetable

```{r}
metadata <- c(
  lp = "//legislativePeriod",
  session = "//titleStmt/sessionNo",
  date = "//publicationStmt/date"
)
P$xmlToBasetable(metadata = metadata)
```

Some adjustments and some cleaning.

```{r}
P$basetable <- P$basetable[is.na(speaker)][, speaker := NULL] # remove text in speaker tag
P$basetable[, chunk := 1:nrow(P$basetable)] # add column with chunks
for (x in c("div_what", "div_desc", "body", "TEI", "p")) P$basetable[[x]] <- NULL # remove unnecessary columns
P$basetable[["stage_type"]] <- ifelse(is.na(dt2[["stage_type"]]), "speech", "interjection")
```


```{r}
P$dissectMegatable()
P$size()
```


# Annotation using Stanford CoreNLP

```{r}
P$texttable <- data.table::fread("~/Lab/tmp/testpipe/csv/texttable.csv")
P$threads <- 1L
P$corenlp()
```

# NDJSON to csv

We have 'ndjson' output at this stage. This needs to be turned into a vertical / tabular format. No parallelization here so far, as the procedure is sufficiently fast (~ 1,5 h).

```{r}
P$ndjsonToDf()
```


# Turn data.table into a CWB indexed corpus

```{r}
options("polmineR.cwb-regedit" = FALSE)

P$tokenstream <-  data.table::fread(file.path(P$dir, "csv", "tokenstream.csv"))
P$metadata <- data.table::fread(file.path(P$dir, "csv", "metadata.csv"))
setnames(P$metadata, old = c("sp_party", "sp_name"), new = c("party", "name"))

P$addCposToMetadataTable()
P$encodePAttributes("FOO", cols = c("word", "pos"))
P$encodeSAttributes("FOO", cols = c("party", "name", "lp", "session", "date"))
```


```{r}
use() # to make the new corpus available
```


# Annex

# Annotation: Without parallelization

Initialize annotator

```{r}
CNLP <- CoreNLP$new(
  method = "json", filename = "~/Lab/tmp/coreNLP.json",
  stanfordDir = stanfordDir, propertiesFile = propertiesFile
  )
```

```{r}
dummy <- pbapply::pblapply(
  1:nrow(dt2),
  function(i) CNLP$annotate(dt2[["text"]][i], chunk = i) # add chunks for matching with metadata table
)
```

```{r}
J <- readr::read_lines(file = "~/Lab/tmp/coreNLP.json", progress = TRUE)
```

```{r}
dts <- pbapply::pblapply(J, CNLP$parseJson) # parallelization works nicely here
tokenStreamDT <- rbindlist(dts)
```




```{r}
started <- Sys.time()
dfs <- lapply(
filenames,
function(filename){
  con <- file(filename, open = "r")
  df <- jsonlite::stream_in(con)
  close(con)
  do.call(rbind, lapply(df[,2], function(x) do.call(rbind, x[["tokens"]])))
}
)
Sys.time() - started
```


# Annotation using Stanford CoreNLP

It is best to run this in a screen (11 core: ~ 2h)

For some unknown reason, using the CoreNLP class in parallel mode when ctk is loaded as a package does not work properly. Therefore, the class is courced in.

```{r}
library(rJava)
source("~/Lab/github/ctk/R/CoreNLP_class.R")
```

```{r}
dtText <- P$texttable
chunks <- text2vec::split_into(1:nrow(dtText), n = noCores)
filenames <- sprintf(
  file.path(P$dir, "ndjson", "corenlp_%d.ndjson"),
  1:noCores
  )
```

```{r}
started <- Sys.time()
dummy <- lapply(
  1:length(chunks),
  function(i){
    options(java.parameters = "-Xmx4g")
    A <- CoreNLP$new(
      method = "json", filename = filenames[i],
      stanfordDir = stanfordDir, propertiesFile = propertiesFile
      )
    lapply(chunks[[i]], function(j) A$annotate(dtText[["text"]][j], chunk = j))
    return( filename )
  }
  # , mc.cores = noCores
)
ended <- Sys.time()
ended - started
```

